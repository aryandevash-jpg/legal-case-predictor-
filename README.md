# âš–ï¸ Legal Case Predictor

An AI-powered legal case prediction system that uses LegalBERT to predict case verdicts and generate summaries. This application provides both a REST API and a beautiful web interface for legal case analysis.

![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104.1-green.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

## ğŸ¯ Features

- **AI-Powered Predictions**: Uses fine-tuned LegalBERT model to predict case verdicts
- **Confidence Scores**: Provides probability distributions for predictions
- **Case Summarization**: Automatically generates concise summaries using BART
- **Verdict Reasoning**: Explains the prediction with key factors and supporting evidence
- **Interactive Web Interface**: Beautiful, modern UI for easy case analysis
- **RESTful API**: Well-documented API endpoints for integration

## ğŸ“‹ Prerequisites

- Python 3.10 or higher
- pip (Python package manager)
- Git (for cloning the repository)

## ğŸš€ Quick Start

### Step 1: Clone the Repository

```bash
git clone https://github.com/aryandevash-jpg/legal-case-predictor-.git
cd legal-case-predictor-
```

### Step 2: Create Virtual Environment

Create a virtual environment to isolate project dependencies:

```bash
# On macOS/Linux
python3 -m venv venv

# On Windows
python -m venv venv
```

### Step 3: Activate Virtual Environment

```bash
# On macOS/Linux
source venv/bin/activate

# On Windows
venv\Scripts\activate
```

### Step 4: Install Dependencies

Install all required packages:

```bash
pip install -r requirements.txt
```

> **Note**: This will download the model files from Hugging Face on first run, which may take a few minutes.

### Step 5: Start the Server

Start the FastAPI server using uvicorn:

```bash
uvicorn predicted_output:app --host 0.0.0.0 --port 8000
```

You should see output like:
```
INFO:     Started server process [xxxxx]
INFO:     Waiting for application startup.
Loading tokenizer from Hugging Face Hub...
Model and tokenizer loaded successfully on cpu
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000
```

### Step 6: Access the Application

Open your web browser and navigate to:

**Option 1: Use the Web Interface (Recommended)**
```
http://localhost:8000
```

**Option 2: Open index.html directly**
```
Open index.html in your browser (file:///path/to/index.html)
```

The web interface provides an intuitive way to:
- Enter legal case text
- View predictions with confidence scores
- See detailed reasoning for verdicts
- Read automatically generated summaries

## ğŸ“¡ API Endpoints

The API provides three main endpoints:

### 1. Health Check

Check the API status and model availability.

**Endpoint:** `GET /health`

**Response:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "device": "cpu"
}
```

**Example:**
```bash
curl http://localhost:8000/health
```

### 2. API Documentation

Interactive API documentation with Swagger UI.

**Endpoint:** `GET /docs`

**Access:** Open in browser: `http://localhost:8000/docs`

This provides:
- Interactive API testing interface
- Request/response schemas
- Try-it-out functionality

### 3. Predict Verdict

Predict the verdict for a legal case text.

**Endpoint:** `POST /predict`

**Request Body:**
```json
{
  "text": "Your legal case text here...",
  "max_summary_length": 200,
  "min_summary_length": 60,
  "include_reasoning": true
}
```

**Response:**
```json
{
  "pred_label": 0,
  "pred_verdict": "Negative verdict (e.g., Appeal Dismissed / Not Guilty)",
  "probability": 0.85,
  "all_probabilities": [0.85, 0.15],
  "summary": "Case summary generated by BART...",
  "reasoning": {
    "confidence_level": "high",
    "confidence_score": 0.85,
    "key_factors": ["factor1", "factor2", "factor3"],
    "supporting_evidence": [
      "Key sentence 1...",
      "Key sentence 2..."
    ],
    "explanation": "The model predicts a negative verdict with high confidence..."
  }
}
```

**Example using curl:**
```bash
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "The appellant filed an appeal against the lower court decision...",
    "include_reasoning": true
  }'
```

**Example using Python:**
```python
import requests

response = requests.post(
    "http://localhost:8000/predict",
    json={
        "text": "Your legal case text here...",
        "include_reasoning": True
    }
)

result = response.json()
print(f"Predicted Verdict: {result['pred_verdict']}")
print(f"Confidence: {result['probability']:.2%}")
```

## ğŸ—ï¸ Project Structure

```
legal-case-predictor/
â”œâ”€â”€ predicted_output.py      # FastAPI application and model logic
â”œâ”€â”€ index.html               # Web interface frontend
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ train_ljp_transformer.py # Model training script
â”œâ”€â”€ upload_model.py          # Model upload utility
â”œâ”€â”€ render.yaml             # Deployment configuration
â””â”€â”€ README.md               # This file
```

## ğŸ”§ Configuration

### Environment Variables

You can configure the model path using environment variables:

```bash
export MODEL_DIR="AryanJangde/legal-case-predictor-model"
```

Or set it in your code:
```python
import os
os.environ["MODEL_DIR"] = "your-model-path"
```

### Model Information

- **Base Model**: LegalBERT (nlpaueb/legal-bert-base-uncased)
- **Task**: Binary classification (Negative/Positive verdict)
- **Summarization**: BART-large-cnn
- **Max Input Length**: 512 tokens

## ğŸ¨ Web Interface Features

The web interface (`index.html`) includes:

- âœ¨ **Modern UI**: Beautiful gradient design with smooth animations
- ğŸ“Š **Real-time Predictions**: Instant results with visual feedback
- ğŸ“ˆ **Confidence Visualization**: Progress bars and probability distributions
- ğŸ” **Detailed Reasoning**: Key factors and supporting evidence
- ğŸ“ **Case Summarization**: Automatic text summarization
- âœ… **API Health Monitoring**: Real-time connection status

## ğŸ› ï¸ Development

### Running in Development Mode

For auto-reload during development:

```bash
uvicorn predicted_output:app --host 0.0.0.0 --port 8000 --reload
```

### Testing the API

Test all endpoints:

```bash
# Health check
curl http://localhost:8000/health

# Prediction
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "Test case text"}'
```

## ğŸ“¦ Dependencies

Key dependencies include:

- **FastAPI**: Modern web framework for building APIs
- **Uvicorn**: ASGI server for FastAPI
- **Transformers**: Hugging Face transformers library
- **PyTorch**: Deep learning framework
- **Pydantic**: Data validation using Python type annotations

See `requirements.txt` for the complete list.

## ğŸš¢ Deployment

### Using Render

The project includes a `render.yaml` configuration for easy deployment on Render:

1. Connect your GitHub repository to Render
2. Render will automatically detect the configuration
3. The service will be deployed with the specified settings

### Manual Deployment

For other platforms:

1. Set environment variables
2. Install dependencies: `pip install -r requirements.txt`
3. Start server: `uvicorn predicted_output:app --host 0.0.0.0 --port $PORT`

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ‘¤ Author

**Aryan Jangde**

- GitHub: [@aryandevash-jpg](https://github.com/aryandevash-jpg)
- Model: [AryanJangde/legal-case-predictor-model](https://huggingface.co/AryanJangde/legal-case-predictor-model)

## ğŸ™ Acknowledgments

- LegalBERT model by [nlpaueb](https://huggingface.co/nlpaueb/legal-bert-base-uncased)
- BART summarization model by Facebook AI
- FastAPI framework
- Hugging Face Transformers library

## ğŸ“ Support

If you encounter any issues or have questions:

1. Check the [API Documentation](http://localhost:8000/docs) when the server is running
2. Review the health endpoint: `GET /health`
3. Open an issue on GitHub

---

**Happy Predicting! âš–ï¸âœ¨**
